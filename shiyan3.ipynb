{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e895822d-3835-4bd4-a35a-85febd7c5546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上港5-4恒大5分领跑剑指冠军，下轮打平便可夺冠，武磊平纪录 — 广州恒大淘宝|上海上港|蔡慧康|武磊|胡尔克|张成林|阿兰|保利尼奥|王燊超|吕文君|懂球帝                                            北京时间11月3日19:35，中超第28轮迎来天王山之战，广州恒大淘宝坐镇主场迎战上海上港。上半场吕文君和蔡慧康先后进球两度为上港取得领先，保利尼奥和阿兰两度为恒大将比分扳平，补时阶段保利尼奥进球反超比分；下半场武磊进球追平李金羽单赛季进球纪录，王燊超造成张成林乌龙，胡尔克点射破门，阿兰补时打进点球。最终，上海上港客场5-4战胜广州恒大淘宝，赛季双杀恒大同时也将积分榜上的领先优势扩大到五分，上港下轮只要战平就将夺得冠军。                         非常抱歉！\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def filter_tags(htmlstr):\n",
    "    # 过滤DOCTYPE声明\n",
    "    htmlstr = re.sub(r'<!DOCTYPE.*?>', '', htmlstr, flags=re.S)\n",
    "\n",
    "    # 过滤CDATA\n",
    "    htmlstr = re.sub(r'<!\\[CDATA\\[.*?\\]\\]>', '', htmlstr, flags=re.S)\n",
    "\n",
    "    # 过滤<script>标签及内容\n",
    "    htmlstr = re.sub(r'<script.*?>.*?</script>', '', htmlstr, flags=re.S)\n",
    "\n",
    "    # 过滤<style>标签及内容\n",
    "    htmlstr = re.sub(r'<style.*?>.*?</style>', '', htmlstr, flags=re.S)\n",
    "\n",
    "    # 处理换行符，替换为单个换行符\n",
    "    htmlstr = re.sub(r'\\r\\n|\\r|\\n', '', htmlstr)\n",
    "\n",
    "    # 过滤HTML标签\n",
    "    htmlstr = re.sub(r'<.*?>', '', htmlstr)\n",
    "\n",
    "    # 过滤HTML注释\n",
    "    htmlstr = re.sub(r'<!--.*?-->', '', htmlstr, flags=re.S)\n",
    "\n",
    "    # 剔除超链接的内容，保留超链接文本\n",
    "    htmlstr = re.sub(r'<a[^>]*>(.*?)</a>', r'\\1', htmlstr, flags=re.S)\n",
    "\n",
    "    # 清除多余的空行\n",
    "    htmlstr = re.sub(r'\\n+', '\\n', htmlstr)\n",
    "\n",
    "    return htmlstr.strip()  # 去掉首尾多余的空格和换行\n",
    "\n",
    "\n",
    "def readTxt(path):\n",
    "    res = ''\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        res = f.read()\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    str_doc = readTxt(r'htmldome.txt')\n",
    "    s = filter_tags(str_doc)\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d450dce-07e2-4ced-9d5b-15c49c3baf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:\n",
      " 马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军\n",
      "　　记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有摆脱雨水的困扰。7月31日下午6点，国奥队的日常训练再度受到大雨的干扰，无奈之下队员们只慢跑了25分钟就草草收场。\n",
      "　　31日上午10点，国奥队在奥体中心外场训练的时候，天就是阴沉沉的，气象预报显示当天下午沈阳就有大雨，但幸好队伍上午的训练并没有受到任何干扰。\n",
      "　　下午6点，当球队抵达训练场时，大雨已经下了几个小时，而且丝毫没有停下来的意思。抱着试一试的态度，球队开始了当天下午的例行训练，25分钟过去了，天气没有任何转好的迹象，为了保护球员们，国奥队决定中止当天的训练，全队立即返回酒店。\n",
      "　　在雨中训练对足球队来说并不是什么稀罕事，但在奥运会即将开始之前，全队变得“娇贵”了。在沈阳最后一周的训练，国奥队首先要保证现有的球员不再出现意外的伤病情况以免影响正式比赛，因此这一阶段控制训练受伤、控制感冒等疾病的出现被队伍放在了相当重要的位置。而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。队伍介绍说，冯萧霆并没有出现发烧症状，但为了安全起见，这两天还是让他静养休息，等感冒彻底好了之后再恢复训练。由于有了冯萧霆这个例子，因此国奥队对雨中训练就显得特别谨慎，主要是担心球员们受凉而引发感冒，造成非战斗减员。而女足队员马晓旭在热身赛中受伤导致无缘奥运的前科，也让在沈阳的国奥队现在格外警惕，“训练中不断嘱咐队员们要注意动作，我们可不能再出这样的事情了。”一位工作人员表示。\n",
      "　　从长春到沈阳，雨水一路伴随着国奥队，“也邪了，我们走到哪儿雨就下到哪儿，在长春几次训练都被大雨给搅和了，没想到来沈阳又碰到这种事情。”一位国奥球员也对雨水的“青睐”有些不解。\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\max\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.671 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果:\n",
      " ['马晓旭', '意外', '受伤', '国奥', '警惕', '无奈', '大雨', '青睐', '殷家', '记者', '傅亚雨', '沈阳', '报道', '来到', '沈阳', '国奥队', '依然', '摆脱', '雨水', '困扰', '下午', '国奥队', '日常', '训练', '再度', '大雨', '干扰', '无奈', '之下', '队员', '慢跑', '分钟', '草草收场', '上午', '国奥队', '奥体中心', '外场', '训练', '阴沉沉', '气象预报', '显示', '当天', '下午', '沈阳', '大雨', '幸好', '队伍', '上午', '训练', '干扰', '下午', '球队', '抵达', '训练场', '大雨', '几个', '小时', '丝毫', '停下来', '试一试', '态度', '球队', '当天', '下午', '例行', '训练', '分钟', '天气', '转好', '迹象', '保护', '球员', '国奥队', '中止', '当天', '训练', '全队', '返回', '酒店', '训练', '足球队', '稀罕', '奥运会', '全队', '变得', '娇贵', '沈阳', '一周', '训练', '国奥队', '保证', '现有', '球员', '出现意外', '伤病', '情况', '影响', '正式', '比赛', '这一', '阶段', '控制', '训练', '受伤', '控制', '感冒', '疾病', '队伍', '放在', '位置', '抵达', '沈阳', '后卫', '冯萧霆', '训练', '冯萧霆', '长春', '患上', '感冒', '参加', '塞尔维亚', '热身赛', '队伍', '介绍', '冯萧霆', '发烧', '症状', '两天', '静养', '休息', '感冒', '恢复', '训练', '冯萧霆', '例子', '国奥队', '对雨中', '训练', '显得', '特别', '谨慎', '担心', '球员', '受凉', '引发', '感冒', '非战斗', '减员', '女足', '队员', '马晓旭', '热身赛', '受伤', '导致', '无缘', '奥运', '前科', '沈阳', '国奥队', '警惕', '训练', '嘱咐', '队员', '动作', '再出', '事情', '一位', '工作人员', '长春', '沈阳', '雨水', '一路', '伴随', '国奥队', '长春', '几次', '训练', '大雨', '搅和', '没想到', '沈阳', '碰到', '事情', '一位', '国奥', '球员', '雨水', '青睐', '不解']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "# 利用jieba对文本进行分词，返回切词后的list\n",
    "def seg_doc(str_doc):\n",
    "    # 1 正则处理原文本，清洗数据\n",
    "    clean_doc = textParse(str_doc)\n",
    "    \n",
    "    # 2 获取停用词列表\n",
    "    stop_words = get_stop_words()\n",
    "    \n",
    "    # 3 分词并去除停用词\n",
    "    words = jieba.lcut(clean_doc)\n",
    "    \n",
    "    # 4 去掉停用词、数字、单个字符等无意义词汇\n",
    "    filtered_words = rm_tokens(words, stop_words)\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "# 正则对字符串进行清洗，去除空格、换行符、特殊符号等\n",
    "def textParse(str_doc):\n",
    "    # 去掉所有特殊字符，仅保留中文字符、字母和数字\n",
    "    str_doc = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9]', ' ', str_doc)\n",
    "    # 去除多余的空格\n",
    "    str_doc = re.sub(r'\\s+', ' ', str_doc)\n",
    "    return str_doc.strip()\n",
    "\n",
    "\n",
    "# 创建停用词列表\n",
    "def get_stop_words(path=r'stopwords.txt'):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        stop_words = f.read().split('\\n')\n",
    "    return set(stop_words)\n",
    "\n",
    "\n",
    "# 去掉无意义的词汇（停用词、数字、单个字符）\n",
    "def rm_tokens(words, stwlist):\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        # 去除停用词、数字、单个字符和空字符\n",
    "        if word not in stwlist and len(word) > 1 and not word.isdigit():\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "# 读取文本文件内容\n",
    "def readFile(path):\n",
    "    str_doc = \"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        str_doc = f.read()\n",
    "    return str_doc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1 读取文本\n",
    "    path = r'0.txt'\n",
    "    str_doc = readFile(path)\n",
    "    print(\"原始文本:\\n\", str_doc)\n",
    "    \n",
    "    # 2 分词并去除停用词\n",
    "    word_list = seg_doc(str_doc)\n",
    "    print(\"分词结果:\\n\", word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b719c9-7cee-48b1-9470-85daed9bf4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF特征选择结果:\n",
      "('to', 0.0322394037469742)\n",
      "('stop', 0.0322394037469742)\n",
      "('worthless', 0.0322394037469742)\n",
      "('my', 0.028288263356383563)\n",
      "('dog', 0.028288263356383563)\n",
      "('him', 0.028288263356383563)\n",
      "('stupid', 0.028288263356383563)\n",
      "('has', 0.025549122992281622)\n",
      "('flea', 0.025549122992281622)\n",
      "('problems', 0.025549122992281622)\n",
      "('help', 0.025549122992281622)\n",
      "('please', 0.025549122992281622)\n",
      "('maybe', 0.025549122992281622)\n",
      "('not', 0.025549122992281622)\n",
      "('take', 0.025549122992281622)\n",
      "('park', 0.025549122992281622)\n",
      "('dalmation', 0.025549122992281622)\n",
      "('is', 0.025549122992281622)\n",
      "('so', 0.025549122992281622)\n",
      "('cute', 0.025549122992281622)\n",
      "('I', 0.025549122992281622)\n",
      "('love', 0.025549122992281622)\n",
      "('posting', 0.025549122992281622)\n",
      "('garbage', 0.025549122992281622)\n",
      "('mr', 0.025549122992281622)\n",
      "('licks', 0.025549122992281622)\n",
      "('ate', 0.025549122992281622)\n",
      "('steak', 0.025549122992281622)\n",
      "('how', 0.025549122992281622)\n",
      "('quit', 0.025549122992281622)\n",
      "('buying', 0.025549122992281622)\n",
      "('food', 0.025549122992281622)\n",
      "\n",
      "特征词总数: 32\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# 加载数据集函数\n",
    "def loadDataSet():\n",
    "    dataset = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 类别标签向量，1代表不好，0代表好\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return dataset, classVec\n",
    "\n",
    "\n",
    "# 计算TF-IDF值的函数\n",
    "def feature_select(list_words):\n",
    "    # 1. 统计总词频\n",
    "    doc_count = len(list_words)  # 文档总数\n",
    "    word_count = defaultdict(int)  # 统计每个词的总频率\n",
    "    doc_freq = defaultdict(int)  # 统计包含某个词的文档数\n",
    "\n",
    "    for doc in list_words:\n",
    "        # 每篇文档中的词频统计\n",
    "        unique_words_in_doc = set(doc)\n",
    "        for word in doc:\n",
    "            word_count[word] += 1\n",
    "        # 统计包含某词的文档数（文档词去重后计算）\n",
    "        for word in unique_words_in_doc:\n",
    "            doc_freq[word] += 1\n",
    "\n",
    "    # 2. 计算每个词的TF值和IDF值\n",
    "    tf_idf_dict = {}\n",
    "    for word in word_count:\n",
    "        # 计算TF值：词在文档中的频率\n",
    "        tf = word_count[word] / sum(word_count.values())\n",
    "        # 计算IDF值：log(总文档数 / 包含该词的文档数)\n",
    "        idf = math.log(doc_count / (doc_freq[word] + 1))  # +1是为了防止分母为0\n",
    "        # 计算TF-IDF\n",
    "        tf_idf_dict[word] = tf * idf\n",
    "\n",
    "    # 3. 对字典按TF-IDF值从大到小排序\n",
    "    sorted_tf_idf = sorted(tf_idf_dict.items(),\n",
    "                            key=lambda item: item[1], reverse=True)\n",
    "    return sorted_tf_idf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 加载数据集\n",
    "    data_list, label_list = loadDataSet()\n",
    "\n",
    "    # 计算所有词的TF-IDF值\n",
    "    features = feature_select(data_list)\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"TF-IDF特征选择结果:\")\n",
    "    for feature in features:\n",
    "        print(feature)\n",
    "\n",
    "    # 打印特征词的数量\n",
    "print(\"\\n特征词总数:\", len(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6cb164-6b9f-4417-807b-03a79f96d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集:\n",
      " [['姚明' '我来' '承担' '连败' '巨人' '宣言' '酷似' '当年' '麦蒂' '新浪' '体育讯' '北京' '时间' '消息'\n",
      "  '休斯敦' '纪事报' '专栏' '记者' '乔纳森' '费根' '报道' '姚明' '渴望' '一场' '胜利' '当年' '队友'\n",
      "  '麦蒂' '惯用' '句式']\n",
      " ['谢婷婷' '模特' '酬劳' '仅够' '生活' '风光' '背后' '惨遭' '拖薪' '新浪' '娱乐' '金融' '海啸'\n",
      "  'blog' '席卷' '全球' '模特儿' '酬劳' '被迫' '打折' '全职' 'Model' '谢婷婷' '业界' '工作量'\n",
      "  '有增无减' '收入' '仅够' '糊口' '拖薪']\n",
      " ['名师' '解读' '四六级' '阅读' '真题' '技巧' '考前' '复习' '重点' '历年' '真题' '阅读' '听力' '完形'\n",
      "  '提升' '空间' '天中' '题为' '主导' '考过' '六级' '四级' '题为' '主导' '真题' '告诉' '方向' '会考'\n",
      "  '题材' '包括']\n",
      " ['美国' '军舰' '抵达' '越南' '联合' '军演' '中新社' '北京' '日电' '杨刚' '美国' '海军' '第七' '舰队'\n",
      "  '三艘' '军舰' '抵达' '越南' '岘港' '为期' '七天' '美越' '南海' '联合' '军事训练' '拉开序幕' '美国'\n",
      "  '海军' '官方网站' '消息']] \n",
      "标签集:\n",
      " [['体育' '娱乐' '教育' '时政']]\n",
      "\n",
      "词汇列表：\n",
      " ['胜利', '杨刚', '题为', '美国', '新浪', '七天', '完形', '惯用', '酬劳', '风光', '四级', '技巧', '句式', '四六级', '名师', '军事训练', '军舰', '舰队', '连败', '主导', '休斯敦', '全职', '乔纳森', '考过', '消息', '当年', '费根', 'Model', '模特', '记者', '拉开序幕', '真题', '为期', '三艘', '拖薪', '会考', '阅读', '考前', '糊口', 'blog', '空间', '复习', '提升', '北京', '金融', '官方网站', '美越', '包括', '全球', '谢婷婷', '题材', '酷似', '麦蒂', '渴望', '生活', '联合', '工作量', '业界', '报道', '背后', '抵达', '娱乐', '解读', '惨遭', '一场', '时间', '承担', '重点', '越南', '队友', '专栏', '岘港', '南海', '日电', '纪事报', '海啸', '体育讯', '海军', '历年', '告诉', '席卷', '打折', '巨人', '军演', '听力', '有增无减', '方向', '我来', '被迫', '中新社', '第七', '模特儿', '仅够', '六级', '收入', '天中', '宣言', '姚明']\n",
      "词袋模型:\n",
      " [[1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 2 1 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 2 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0\n",
      "  0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 2]\n",
      " [0 0 0 0 1 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 2 0\n",
      "  0 0 1 1 0 0 0 0 1 0 0 0 1 2 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 2 0 1 0 0 0]\n",
      " [0 0 2 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 3 0 0 0 1\n",
      "  2 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 1 0 3 0 1 0 0 0 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0\n",
      "  0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 1\n",
      "  1 1 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0]]\n",
      "tf-idf:\n",
      " [[0.02310491 0.         0.         0.         0.0095894  0.\n",
      "  0.         0.02310491 0.         0.         0.         0.\n",
      "  0.02310491 0.         0.         0.         0.         0.\n",
      "  0.02310491 0.         0.02310491 0.         0.02310491 0.\n",
      "  0.0095894  0.0191788  0.02310491 0.         0.         0.02310491\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.0095894  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02310491 0.0191788  0.02310491\n",
      "  0.         0.         0.         0.         0.02310491 0.\n",
      "  0.         0.         0.         0.         0.02310491 0.02310491\n",
      "  0.02310491 0.         0.         0.02310491 0.02310491 0.\n",
      "  0.         0.         0.02310491 0.         0.02310491 0.\n",
      "  0.         0.         0.         0.         0.02310491 0.\n",
      "  0.         0.         0.         0.02310491 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02310491 0.0191788 ]\n",
      " [0.         0.         0.         0.         0.0095894  0.\n",
      "  0.         0.         0.0191788  0.02310491 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02310491 0.         0.\n",
      "  0.         0.         0.         0.02310491 0.02310491 0.\n",
      "  0.         0.         0.         0.         0.0191788  0.\n",
      "  0.         0.         0.02310491 0.02310491 0.         0.\n",
      "  0.         0.         0.02310491 0.         0.         0.\n",
      "  0.02310491 0.0191788  0.         0.         0.         0.\n",
      "  0.02310491 0.         0.02310491 0.02310491 0.         0.02310491\n",
      "  0.         0.02310491 0.         0.02310491 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02310491 0.         0.\n",
      "  0.         0.         0.02310491 0.02310491 0.         0.\n",
      "  0.         0.02310491 0.         0.         0.02310491 0.\n",
      "  0.         0.02310491 0.0191788  0.         0.02310491 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.0191788  0.         0.         0.\n",
      "  0.02310491 0.         0.         0.         0.02310491 0.02310491\n",
      "  0.         0.02310491 0.02310491 0.         0.         0.\n",
      "  0.         0.0191788  0.         0.         0.         0.02310491\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02310491\n",
      "  0.0191788  0.02310491 0.         0.         0.02310491 0.02310491\n",
      "  0.02310491 0.         0.         0.         0.         0.02310491\n",
      "  0.         0.         0.02310491 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.02310491 0.         0.         0.\n",
      "  0.         0.02310491 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02310491 0.02310491 0.         0.         0.         0.\n",
      "  0.02310491 0.         0.02310491 0.         0.         0.\n",
      "  0.         0.         0.         0.02310491 0.         0.02310491\n",
      "  0.         0.        ]\n",
      " [0.         0.02310491 0.         0.         0.         0.02310491\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02310491 0.0191788  0.02310491\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0095894  0.         0.         0.         0.         0.\n",
      "  0.02310491 0.         0.02310491 0.02310491 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.0095894  0.         0.02310491 0.02310491 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.0191788  0.         0.         0.         0.\n",
      "  0.0191788  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.0191788  0.         0.         0.02310491\n",
      "  0.02310491 0.02310491 0.         0.         0.         0.0191788\n",
      "  0.         0.         0.         0.         0.         0.02310491\n",
      "  0.         0.         0.         0.         0.         0.02310491\n",
      "  0.02310491 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import mat\n",
    "import math\n",
    "\n",
    "# 1. 创建数据集：单词列表和所属类别\n",
    "def loadDataSet():\n",
    "    corpus = []\n",
    "    tiyu = ['姚明', '我来', '承担', '连败', '巨人', '宣言', '酷似', '当年', '麦蒂', '新浪', '体育讯', '北京', '时间', '消息', '休斯敦', '纪事报', '专栏', '记者', '乔纳森', '费根', '报道', '姚明', '渴望', '一场', '胜利', '当年', '队友', '麦蒂', '惯用', '句式']\n",
    "    yule = ['谢婷婷', '模特', '酬劳', '仅够', '生活', '风光', '背后', '惨遭', '拖薪', '新浪', '娱乐', '金融', '海啸', 'blog', '席卷', '全球', '模特儿', '酬劳', '被迫', '打折', '全职', 'Model', '谢婷婷', '业界', '工作量', '有增无减', '收入', '仅够', '糊口', '拖薪']\n",
    "    jioayu = ['名师', '解读', '四六级', '阅读', '真题', '技巧', '考前', '复习', '重点', '历年', '真题', '阅读', '听力', '完形', '提升', '空间', '天中', '题为', '主导', '考过', '六级', '四级', '题为', '主导', '真题', '告诉', '方向', '会考', '题材', '包括']\n",
    "    shizheng = ['美国', '军舰', '抵达', '越南', '联合', '军演', '中新社', '北京', '日电', '杨刚', '美国', '海军', '第七', '舰队', '三艘', '军舰', '抵达', '越南', '岘港', '为期', '七天', '美越', '南海', '联合', '军事训练', '拉开序幕', '美国', '海军', '官方网站', '消息']\n",
    "\n",
    "    corpus.append(tiyu)\n",
    "    corpus.append(yule)\n",
    "    corpus.append(jioayu)\n",
    "    corpus.append(shizheng)\n",
    "    classVec = ['体育', '娱乐', '教育', '时政']\n",
    "    return corpus, classVec\n",
    "\n",
    "# 2. 获取所有单词的集合: 返回不含重复元素的单词列表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  # 创建一个空集\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  # 取并集\n",
    "    return list(vocabSet)\n",
    "\n",
    "# 3. 文档词袋模型构建数据矩阵\n",
    "def bagOfWords2VecMN(vocabList, dataSet):\n",
    "    bagVec = []\n",
    "    for document in dataSet:\n",
    "        wordVec = [0] * len(vocabList)  # 创建与词汇表等长的向量\n",
    "        for word in document:\n",
    "            if word in vocabList:\n",
    "                wordVec[vocabList.index(word)] += 1  # 如果单词在词汇表中出现，频次+1\n",
    "        bagVec.append(wordVec)\n",
    "    return bagVec\n",
    "\n",
    "# 4. 词袋模型转化为tf-idf\n",
    "def TFIDF(bagVec):\n",
    "    N = len(bagVec)  # 文档总数\n",
    "    word_count = np.sum(bagVec, axis=0)  # 每个词在整个语料库中的总出现次数\n",
    "\n",
    "    tfidf = np.zeros_like(bagVec, dtype=float)\n",
    "    for i, doc in enumerate(bagVec):\n",
    "        doc_len = sum(doc)\n",
    "        for j in range(len(doc)):\n",
    "            tf = doc[j] / doc_len if doc_len != 0 else 0  # 计算TF\n",
    "            idf = math.log(N / (word_count[j] + 1))  # 计算IDF，+1是为了防止分母为0\n",
    "            tfidf[i][j] = tf * idf  # 计算TF-IDF值\n",
    "    return tfidf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. 打印数据集和标签\n",
    "    dataSet, classlab = loadDataSet()\n",
    "    print('数据集:\\n', mat(dataSet), '\\n标签集:\\n', mat(classlab))\n",
    "    \n",
    "    # 2. 获取所有单词的集合\n",
    "    vocabList = createVocabList(dataSet)\n",
    "    print('\\n词汇列表：\\n', vocabList)\n",
    "    \n",
    "    # 3. 词袋模型: 文本向量转化\n",
    "    bagvec = bagOfWords2VecMN(vocabList, dataSet)\n",
    "    print('词袋模型:\\n', mat(bagvec))\n",
    "    \n",
    "    # 4. tf-idf计算\n",
    "    tfidf = TFIDF(bagvec)\n",
    "print('tf-idf:\\n', mat(tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5858a-302f-4afb-af85-5dd50e6267eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
