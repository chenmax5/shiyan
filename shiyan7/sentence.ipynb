{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2022)\n",
    "\n",
    "\n",
    "# 数据分组\n",
    "def split_list(split_list, rates=[]):\n",
    "    split_target = split_list.copy()\n",
    "    assert np.array(rates).sum() == 1\n",
    "    split_len = [int(r * len(split_list)) for r in rates]\n",
    "    split_result = list()\n",
    "    for index, length in enumerate(split_len):\n",
    "        if index >= len(split_len) - 1:\n",
    "            split_result.append(split_target)\n",
    "            continue\n",
    "        split_ = random.sample(split_target, length)\n",
    "        split_result.append(split_)\n",
    "        split_target = [data for data in split_target if data not in split_]\n",
    "    return split_result\n",
    "\n",
    "\n",
    "def prepare_sst_data():\n",
    "    sst_datasets = list()\n",
    "    data_sentence_name = r\"SST-2/original/datasetSentences.txt\"\n",
    "    data_sentence = pd.read_csv(data_sentence_name, delimiter=\"\\t\", header=0)\n",
    "    dictionary_name = r\"SST-2/original/dictionary.txt\"\n",
    "    dic = pd.read_csv(dictionary_name, delimiter=\"|\", header=None)\n",
    "    dic_maps = {k: v for k, v in zip(dic[0], dic[1])}\n",
    "    label_file = r\"SST-2/original/sentiment_labels.txt\"\n",
    "    label_info = pd.read_csv(label_file, delimiter=\"|\", header=0)\n",
    "    label_map = {\n",
    "        k: v for k, v in zip(label_info[\"phrase ids\"], label_info[\"sentiment values\"])\n",
    "    }\n",
    "    for sentence in tqdm(data_sentence[\"sentence\"]):\n",
    "        if sentence not in dic_maps:\n",
    "            continue\n",
    "        label = label_map[dic_maps[sentence]]\n",
    "        label = 0 if float(label) < 0.5 else 1\n",
    "        sst_datasets.append({\"sentence\": sentence, \"label\": label})\n",
    "    [train, test] = split_list(sst_datasets, [0.8, 0.2])\n",
    "\n",
    "    return sst_datasets, train, test\n",
    "\n",
    "\n",
    "sst_datasets, train_data, test_data = prepare_sst_data()\n",
    "\n",
    "dataset_radio = lambda dataset: round(\n",
    "    np.array([label[\"label\"] for label in dataset]).sum() / len(dataset), 4\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"postive in sst datasets = {dataset_radio(sst_datasets)}, total = {len(sst_datasets)}\"\n",
    ")\n",
    "print(f\"postive in train = {dataset_radio(train_data)}, total = {len(train_data)}\")\n",
    "print(f\"postive in test = {dataset_radio(test_data)}, total = {len(test_data)}\")\n",
    "print(f\"train data[0] = {train_data[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# 分词并去停用词\n",
    "def split_words(sentences: str):\n",
    "    stop_words = [\n",
    "        \"\",\n",
    "        \"the\",\n",
    "        \",\",\n",
    "        \"a\",\n",
    "        \"of\",\n",
    "        \"an\",\n",
    "        \"at\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"in\",\n",
    "        \"as\",\n",
    "        \"'s\",\n",
    "        \".\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"'\",\n",
    "        \"...\",\n",
    "        \"--\",\n",
    "        \"``\",\n",
    "        \"''\",\n",
    "        \"''\",\n",
    "    ]\n",
    "    words = sentences.split(\" \")\n",
    "    return [word.lower() for word in words if word.lower() not in stop_words]\n",
    "\n",
    "\n",
    "all_sentences = [split_words(data[\"sentence\"]) for data in sst_datasets]\n",
    "print(f\"all_sentences[0:2l={all_sentences[0:2]}\")\n",
    "\n",
    "# 训sword2vector词统入模型\n",
    "print(f\"{time.ctime()}: 开始训练word2vector英型\")\n",
    "word2vector = Word2Vec(all_sentences, vector_size=100, min_count=5, epochs=200, sg=0)\n",
    "print(f\"{time.ctime()}:word2vector模型训练结束\")\n",
    "\n",
    "# 保存词向量辕垫\n",
    "W2V_TXT_FILE = \"word2vector_23.txt\"\n",
    "word2vector.wv.save_ord2vec_format(W2V_TXT_FILE)\n",
    "\n",
    "# 根指word2vector模型查看和movie,相近的词\n",
    "print(f\"和movie相近的词如下:\")\n",
    "for i in word2vector.wv.most_similar(\"movies\"):\n",
    "    print(\"--->\", i[0], i[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
