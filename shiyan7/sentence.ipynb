{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (d:\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (d:\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2022)\n",
    "\n",
    "\n",
    "# 数据分组\n",
    "def split_list(split_list, rates=[]):\n",
    "    split_target = split_list.copy()\n",
    "    assert np.array(rates).sum() == 1\n",
    "    split_len = [int(r * len(split_list)) for r in rates]\n",
    "    split_result = list()\n",
    "    for index, length in enumerate(split_len):\n",
    "        if index >= len(split_len) - 1:\n",
    "            split_result.append(split_target)\n",
    "            continue\n",
    "        split_ = random.sample(split_target, length)\n",
    "        split_result.append(split_)\n",
    "        split_target = [data for data in split_target if data not in split_]\n",
    "    return split_result\n",
    "\n",
    "\n",
    "def prepare_sst_data():\n",
    "    sst_datasets = list()\n",
    "    data_sentence_name = r\"SST-2/original/datasetSentences.txt\"\n",
    "    data_sentence = pd.read_csv(data_sentence_name, delimiter=\"\\t\", header=0)\n",
    "    dictionary_name = r\"SST-2/original/dictionary.txt\"\n",
    "    dic = pd.read_csv(dictionary_name, delimiter=\"|\", header=None)\n",
    "    dic_maps = {k: v for k, v in zip(dic[0], dic[1])}\n",
    "    label_file = r\"SST-2/original/sentiment_labels.txt\"\n",
    "    label_info = pd.read_csv(label_file, delimiter=\"|\", header=0)\n",
    "    label_map = {\n",
    "        k: v for k, v in zip(label_info[\"phrase ids\"], label_info[\"sentiment values\"])\n",
    "    }\n",
    "    for sentence in tqdm(data_sentence[\"sentence\"]):\n",
    "        if sentence not in dic_maps:\n",
    "            continue\n",
    "        label = label_map[dic_maps[sentence]]\n",
    "        label = 0 if float(label) < 0.5 else 1\n",
    "        sst_datasets.append({\"sentence\": sentence, \"label\": label})\n",
    "    [train, test] = split_list(sst_datasets, [0.8, 0.2])\n",
    "\n",
    "    return sst_datasets, train, test\n",
    "\n",
    "\n",
    "sst_datasets, train_data, test_data = prepare_sst_data()\n",
    "\n",
    "dataset_radio = lambda dataset: round(\n",
    "    np.array([label[\"label\"] for label in dataset]).sum() / len(dataset), 4\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"postive in sst datasets = {dataset_radio(sst_datasets)}, total = {len(sst_datasets)}\"\n",
    ")\n",
    "print(f\"postive in train = {dataset_radio(train_data)}, total = {len(train_data)}\")\n",
    "print(f\"postive in test = {dataset_radio(test_data)}, total = {len(test_data)}\")\n",
    "print(f\"train data[0] = {train_data[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# 分词并去停用词\n",
    "def split_words(sentences: str):\n",
    "    stop_words = [\n",
    "        \"\",\n",
    "        \"the\",\n",
    "        \",\",\n",
    "        \"a\",\n",
    "        \"of\",\n",
    "        \"an\",\n",
    "        \"at\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"in\",\n",
    "        \"as\",\n",
    "        \"'s\",\n",
    "        \".\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"'\",\n",
    "        \"...\",\n",
    "        \"--\",\n",
    "        \"``\",\n",
    "        \"''\",\n",
    "        \"''\",\n",
    "    ]\n",
    "    words = sentences.split(\" \")\n",
    "    return [word.lower() for word in words if word.lower() not in stop_words]\n",
    "\n",
    "\n",
    "all_sentences = [split_words(data[\"sentence\"]) for data in sst_datasets]\n",
    "print(f\"all_sentences[0:2l={all_sentences[0:2]}\")\n",
    "\n",
    "# 训sword2vector词统入模型\n",
    "print(f\"{time.ctime()}: 开始训练word2vector英型\")\n",
    "word2vector = Word2Vec(all_sentences, vector_size=100, min_count=5, epochs=200, sg=0)\n",
    "print(f\"{time.ctime()}:word2vector模型训练结束\")\n",
    "\n",
    "# 保存词向量辕垫\n",
    "W2V_TXT_FILE = \"word2vector_23.txt\"\n",
    "word2vector.wv.save_ord2vec_format(W2V_TXT_FILE)\n",
    "\n",
    "# 根指word2vector模型查看和movie,相近的词\n",
    "print(f\"和movie相近的词如下:\")\n",
    "for i in word2vector.wv.most_similar(\"movies\"):\n",
    "    print(\"--->\", i[0], i[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
