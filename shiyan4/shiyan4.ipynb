{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df99bfd-6277-4d6f-8170-83ba70579212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from email import parser\n",
    "from email import policy\n",
    "from collections import Counter\n",
    "import re\n",
    "from html import unescape\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import neighbors\n",
    "import urlextract\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd60d57f-2a34-4f7d-9d2d-3e7555f419b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATH = os.path.join('trec07p', 'delay', 'index')\n",
    "DATA_PATH = os.path.join('trec07p', 'data')\n",
    "labels = []\n",
    "filenames = []\n",
    "\n",
    "\n",
    "def create_dataset(index_path):\n",
    "    with open(index_path) as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.split(' ')\n",
    "            labels.append(line[0])\n",
    "            filenames.append(line[1].strip('\\n').split('/')[-1])\n",
    "\n",
    "\n",
    "create_dataset(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3351fbe7-613a-4358-a8e3-b8d1d969c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, i've just updated from the gulus and I check on other mirrors.\n",
      "It seems there is a little typo in /debian/README file\n",
      "\n",
      "Example:\n",
      "http://gulus.usherbrooke.ca/debian/README\n",
      "ftp://ftp.fr.debian.org/debian/README\n",
      "\n",
      "\"Testing, or lenny.  Access this release through dists/testing.  The\n",
      "current tested development snapshot is named etch.  Packages which\n",
      "have been tested in unstable and passed automated tests propogate to\n",
      "this release.\"\n",
      "\n",
      "etch should be replace by lenny like in the README.html\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "Yan Morin\n",
      "Consultant en logiciel libre\n",
      "yan.morin@savoirfairelinux.com\n",
      "514-994-1556\n",
      "\n",
      "\n",
      "-- \n",
      "To UNSUBSCRIBE, email to debian-mirrors-REQUEST@lists.debian.org\n",
      "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n"
     ]
    }
   ],
   "source": [
    "def load_email(filename, file_path):\n",
    "    with open(os.path.join(file_path, filename), 'rb') as f:\n",
    "        return parser.BytesParser(policy=policy.default).parse(f)\n",
    "\n",
    "\n",
    "raw_emails = [load_email(name, DATA_PATH) for name in filenames]\n",
    "\n",
    "print(raw_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1661cd1b-b200-40c3-8d85-d049e76fab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return 'multipart({})'.format(', '.join([get_email_structure(sub_email) for sub_email in payload]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a15061-2a94-4f20-9eab-83bf37740b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub(r'<[aA]\\s.*?>', 'HYPERLINK', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub(r'<img\\s.*?>', 'IMAGE', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45751444-622a-424a-8b49-06127e4f8569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text/html', 3612), ('text/plain', 2414), ('multipart(text/plain, text/html)', 2080), ('multipart(multipart(text/plain, text/html), image/gif)', 860), ('multipart(multipart(text/plain, text/html), image/jpeg)', 371), ('multipart(text/html)', 205), ('multipart(text/html, image/gif)', 128), ('multipart(text/plain)', 87), ('multipart(text/plain, application/pgp-signature)', 47), ('multipart(text/plain, text/plain)', 24), ('multipart(multipart(text/plain, text/html), image/png)', 21), ('multipart(text/plain, application/x-msdownload)', 15), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/jpeg, application/octet-stream)', 13), ('multipart(multipart(text/plain, text/html))', 13), ('multipart(multipart(text/plain, application/pgp-signature), text/plain)', 11), ('multipart(text/html, text/plain, image/png)', 11), ('multipart(text/plain charset=us-ascii, text/html)', 9), ('multipart(text/plain, text/x-diff)', 7), ('multipart(multipart(text/html, image/gif), application/octet-stream)', 6), ('multipart(text/plain, application/octet-stream)', 5), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/jpeg, image/gif, application/octet-stream, image/gif, image/jpeg, image/gif)', 5), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, application/octet-stream)', 5), ('multipart(multipart(text/plain, text/html), text/plain)', 4), ('multipart(text/html, application/octet-stream)', 4), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(text/html))', 3), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/jpeg, image/jpeg)', 3), ('multipart(multipart(multipart(text/plain, text/html), image/gif), text/plain)', 3), ('multipart(text/html, image/jpeg)', 2), ('multipart(text/plain, text/plain, text/plain, text/plain)', 2), ('multipart(multipart(text/plain, text/html), image/jpg)', 2), ('multipart(text/plain, text/plain, text/plain)', 2), ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)', 2), ('multipart(multipart(text/plain, text/html), application/pgp-signature)', 2), ('multipart(multipart(text/plain, text/x-patch), application/pgp-signature)', 2), ('multipart(multipart(text/html), image/jpeg)', 2), ('multipart(multipart(text/plain), text/plain)', 2), ('multipart(text/plain, multipart(text/plain), text/plain)', 1), ('multipart(text/plain, text/x-patch)', 1), ('multipart(text/plain, text/x-patch, application/pgp-signature)', 1), ('multipart(multipart(text/plain, application/x-pkcs7-signature), text/plain)', 1), ('multipart(text/plain, text/x-log, text/plain)', 1), ('multipart(multipart(text/plain))', 1), ('multipart(text/plain, image/png, text/plain)', 1), ('multipart(text/plain, plain/text)', 1), ('multipart(multipart(text/plain, text/x-patch, text/x-patch), application/pgp-signature)', 1), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/jpeg, image/gif)', 1), ('multipart(text/plain, multipart(text/html))', 1), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, text/html)))', 1), ('multipart(multipart(text/plain, text/html), application/octet-stream)', 1), ('multipart(text/plain, application/octet-stream, application/octet-stream)', 1), ('multipart(text/html, text/richtext)', 1), ('multipart(text/plain, application/x-pkcs7-signature)', 1)]\n"
     ]
    }
   ],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if ctype not in ('text/plain', 'text/html'):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except LookupError:\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == 'text/plain':\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)\n",
    "print(structures_counter(raw_emails).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81c463f-a655-4d76-a2d2-fa10750184d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel', 'pressur', 'perform', 'rise', 'occas', 'hyperlinktri', 'viagra', 'anxieti', 'thing', 'past', 'back', 'old', 'self']\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "token = nltk.stem.SnowballStemmer('english')\n",
    "for single in range(97, 123):\n",
    "    stopwords_list.append(chr(single))\n",
    "extractor = urlextract.URLExtract()\n",
    "\n",
    "\n",
    "def word_split(email):\n",
    "    text = email_to_text(email) or ' '\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "    urls = list(set(extractor.find_urls(text)))\n",
    "    urls.sort(key=lambda item: len(item), reverse=True)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, \"URL\")\n",
    "    text = re.sub(r'\\d+(?:\\.\\d*[eE]\\d+)?', 'NUMBER', text)\n",
    "    content = list(nltk.word_tokenize(text))\n",
    "    all_words = []\n",
    "    for word in content:\n",
    "        if word not in stopwords_list:\n",
    "            word = token.stem(word)\n",
    "            all_words.append(word)\n",
    "    return all_words\n",
    "all_emails = [word_split(data) for data in raw_emails]\n",
    "print(all_emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62c4d670-4786-42dc-be4d-fc73bcefee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = all_emails\n",
    "trainDF['label'] = labels\n",
    "\n",
    "# 将数据集分为测试集和训练集\n",
    "train_data, test_data, train_label, test_label = train_test_split(trainDF['text'], trainDF['label'], random_state=0)\n",
    "\n",
    "# label编码为目标变量,即从字符串转为一个数字\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_label = encoder.fit_transform(train_label)\n",
    "test_label = encoder.fit_transform(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831c7a99-46d6-446b-9360-784aa1b49cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分词后的列表重新拼接成字符串\n",
    "trainDF['text'] = trainDF['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 确保 train_data 和 test_data 也是字符串形式\n",
    "train_data = [' '.join(doc) for doc in train_data]\n",
    "test_data = [' '.join(doc) for doc in test_data]\n",
    "\n",
    "# 然后再进行向量化\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "count_vect.fit(trainDF['text'])\n",
    "xtrain_count = count_vect.transform(train_data)  # 训练集特征向量\n",
    "xtest_count = count_vect.transform(test_data)    # 测试集特征向量\n",
    "\n",
    "# 4.2 TF-IDF特征向量\n",
    "# 4.2.1 词语级\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "\n",
    "# 4.2.2 多词语级\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "\t\t\t\t\t\t\tngram_range=(2, 3), max_features=5000)\n",
    "\n",
    "# 4.2.3 词性级\n",
    "tfidf_vect_char = TfidfVectorizer(analyzer='char', ngram_range=(2, 3), max_features=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a41329f-857d-4e7f-8a05-5eaf46a6e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建分类器\n",
    "def train_model(classifier, train_feature, test_feature):\n",
    "    classifier.fit(train_feature, train_label)\n",
    "    prediction = classifier.predict(test_feature)\n",
    "    acc = metrics.accuracy_score(prediction, test_label)\n",
    "    prec = metrics.precision_score(prediction, test_label)\n",
    "    rec = metrics.recall_score(prediction, test_label)\n",
    "    f1 = metrics.f1_score(prediction, test_label)\n",
    "    return acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28723c77-aad4-467f-ad3e-56b4544f3a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (Multinomial), Count Vectors: \n",
      "Accuracy: 0.8460, Precision: 0.8104, Recall: 1.0000, F1 Score: 0.8953\n",
      "NB (Bernoulli), Count Vectors: \n",
      "Accuracy: 0.8324, Precision: 0.7937, Recall: 1.0000, F1 Score: 0.8850\n",
      "SVM, Count Vectors: \n",
      "Accuracy: 0.9748, Precision: 0.9916, Recall: 0.9777, F1 Score: 0.9846\n",
      "Random Forest, Count Vectors: \n",
      "Accuracy: 0.9936, Precision: 0.9985, Recall: 0.9936, F1 Score: 0.9961\n",
      "KNN, Count Vectors: \n",
      "Accuracy: 0.9652, Precision: 0.9921, Recall: 0.9660, F1 Score: 0.9789\n"
     ]
    }
   ],
   "source": [
    "# 5.1 朴素贝叶斯多项式模型\n",
    "accuracy, precision, recall, f1_score = train_model(naive_bayes.MultinomialNB(),\n",
    "                            xtrain_count,\n",
    "                            xtest_count)\n",
    "print(\"NB (Multinomial), Count Vectors: \")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# 5.2 朴素贝叶斯伯努利模型\n",
    "accuracy, precision, recall, f1_score = train_model(naive_bayes.BernoulliNB(), xtrain_count, xtest_count)\n",
    "print(\"NB (Bernoulli), Count Vectors: \")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# 5.3 SVM\n",
    "accuracy, precision, recall, f1_score = train_model(svm.SVC(), xtrain_count, xtest_count)\n",
    "print(\"SVM, Count Vectors: \")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# 5.4 随机森林\n",
    "accuracy, precision, recall, f1_score = train_model(ensemble.RandomForestClassifier(), xtrain_count, xtest_count)\n",
    "print(\"Random Forest, Count Vectors: \")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# 5.5 KNN\n",
    "accuracy, precision, recall, f1_score = train_model(neighbors.KNeighborsClassifier(), xtrain_count, xtest_count)\n",
    "print(\"KNN, Count Vectors: \")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
