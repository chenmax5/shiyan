{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ceb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "import xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e71d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'F:/github-project/shiyan/shiyan5/第五章 话题追踪与检测/话题检测/tc-corpus-answer/answer/'\n",
    "dir_names = os.listdir(path_data)\n",
    "labels,texts=[], []\n",
    "for dir_name in dir_names:\n",
    "    file_names =os.listdir(path_data + dir_name +'/')\n",
    "    for file_name in file_names:\n",
    "        f = open(path_data + dir_name + '/'+ file_name, encoding='gb18030', errors='ignore')\n",
    "        content = f.read()\n",
    "        f.close()\n",
    "        labels.append(dir_name)\n",
    "        texts.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba14c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pandas.DataFrame()\n",
    "trainDF[\"text\"] = texts\n",
    "trainDF[\"label\"] = labels\n",
    "train_x,valid_x,train_y,valid_y = model_selection.train_test_split(trainDF[\"text\"],trainDF[\"label\"])\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "count_vect = CountVectorizer(analyzer=\"word\", token_pattern=r\"\\w{1,}\")\n",
    "count_vect.fit(trainDF[\"text\"])\n",
    "xtrain_count = count_vect.transform(train_x)\n",
    "xvalid_count = count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9748f8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:555: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 词语级tf-idf\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    analyzer=\"word\", token_pattern=r\"\\w{1,}\", max_features=5000\n",
    ")\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "# ngram 级tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(\n",
    "    analyzer=\"word\", token_pattern=r\"\\w{1,}\", ngram_range=(2, 3), max_features=5000\n",
    ")\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram = tfidf_vect_ngram.transform(valid_x)\n",
    "# 词性级tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(\n",
    "    analyzer=\"char\", token_pattern=r\"\\w{1,}\", ngram_range=(2, 3), max_features=5000\n",
    ")\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(train_x)\n",
    "xvalid_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02106892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train,label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifien\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions =classifier.predict(feature_vector_valid)\n",
    "    if is_neural_net:\n",
    "        predictions =predictions.argmax(axis=-1)\n",
    "        return metrics.accuracy_score(predictions,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e8821a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB,Count Vectors: None\n",
      "NB,WordLevel TF-IDF: None\n",
      "NB,N-Gram Vectors: None\n",
      "NB,CharLevel Vectors: None\n"
     ]
    }
   ],
   "source": [
    "# 特征为计数向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(),xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB,Count Vectors:\",accuracy)\n",
    "# 特征为词语级别TF-IDF向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB,WordLevel TF-IDF:\",accuracy)\n",
    "#特征为多个词语级别TF-IDF向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB,N-Gram Vectors:\",accuracy)\n",
    "# 特征为词性级别TF-IDF向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB,CharLevel Vectors:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59dadac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR,Count Vectors: None\n",
      "LR,WordLevel TF-IDF: None\n",
      "LR,N-Gram Vectors: None\n",
      "LR,CharLevel Vectors: None\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR,Count Vectors:\",accuracy)\n",
    "#特征为词语级别TF-IDF向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR,WordLevel TF-IDF:\",accuracy)\n",
    "#特征为多个词语级别TF-IDF向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR,N-Gram Vectors:\",accuracy)\n",
    "# 特征为词性级别TF-IDF向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_tfidf_ngram_chars, train_y,xvalid_tfidf_ngram_chars)\n",
    "print(\"LR,CharLevel Vectors:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187a003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM,N-Gram Vectors: None\n",
      "RF,Count Vectors: None\n",
      "RF,WordLevel TF-IDF: None\n"
     ]
    }
   ],
   "source": [
    "# 特征为多个词语级别TF-IDF向量的SVM\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM,N-Gram Vectors:\", accuracy)\n",
    "# 特征为计数向量的RF\n",
    "accuracy = train_model(\n",
    "    ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count\n",
    ")\n",
    "print(\"RF,Count Vectors:\", accuracy)\n",
    "# 特征为词语级别TF-IDF向量的RF\n",
    "accuracy = train_model(\n",
    "    ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf\n",
    ")\n",
    "print(\"RF,WordLevel TF-IDF:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75fb09",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 特征为计数问量的Xgboost\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m      3\u001b[0m     xgboost\u001b[38;5;241m.\u001b[39mXGBClassifier(), xtrain_count\u001b[38;5;241m.\u001b[39mtocsc(), train_y, xvalid_count\u001b[38;5;241m.\u001b[39mtocsc()\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXgb,Count Vectors:\u001b[39m\u001b[38;5;124m\"\u001b[39m,accuracy)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 特征为词语级别TF-IDF向量的Xgboost\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(classifier, feature_vector_train,label, feature_vector_valid, is_neural_net\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# fit the training dataset on the classifien\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mfit(feature_vector_train, label)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# predict the labels on validation dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mpredict(feature_vector_valid)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1532\u001b[0m     params,\n\u001b[0;32m   1533\u001b[0m     train_dmatrix,\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1535\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1536\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1537\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1538\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1539\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1540\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1541\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1543\u001b[0m )\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2103\u001b[0m         )\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 特征为计数问量的Xgboost\n",
    "accuracy = train_model(\n",
    "    xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc()\n",
    ")\n",
    "print(\"Xgb,Count Vectors:\",accuracy)\n",
    "# 特征为词语级别TF-IDF向量的Xgboost\n",
    "accuracy = train_model(\n",
    "    xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc()\n",
    ")\n",
    "print(\"Xgb,WordLevel TF-IDF:\",accuracy)\n",
    "# 特征为词性级别TF-IDF向量的Xgboost\n",
    "accuracy = train_model(\n",
    "    xgboost.XGBClassifier(),\n",
    "    xtrain_tfidf_ngram_chars.tocsc(),\n",
    "    train_y,\n",
    "    xvalid_tfidf_ngram_chars.tocsc(),\n",
    ")\n",
    "print(\"Xgb, CharLevel Vectors:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31592634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer\n",
    "    input_layer = layers.Input((input_size,),sparse=True)\n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifien = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(),loss='binary_crossentropy')\n",
    "    return classifier\n",
    "#浅层神经网络\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print(\"NN,Ngram Level TF IF Vectors\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
